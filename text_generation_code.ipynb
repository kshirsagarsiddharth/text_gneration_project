{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PART 1 \r\n",
    "# reading the data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np \r\n",
    "import pandas as pd \r\n",
    "import matplotlib.pyplot as plt \r\n",
    "import tensorflow as tf "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# opening the file \r\n",
    "path_to_file = 'shakespeare.txt' \r\n",
    "with open(path_to_file,'r') as f:\r\n",
    "    text = f.read() \r\n",
    "\r\n",
    "# creating a vocabulary list \r\n",
    "vocab = sorted(set(text))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "len(vocab) # length of the vocabulary "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART 2 \r\n",
    "# test processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# we want to assign a number to every character in the vocabulary \r\n",
    "char_to_ind = {char:ind for ind,char in enumerate(vocab)} # characters to index \r\n",
    "ind_to_char = np.array(vocab) # index to characters \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "encoded_text = np.array([char_to_ind[char] for char in text])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "seq_len = 120 \r\n",
    "total_num_seq = len(text)  // (seq_len + 1)\r\n",
    "total_num_seq\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "45005"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#char_dataset.batch()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#for item in char_dataset.take(500):\r\n",
    "#    print(ind_to_char[item.numpy()])\r\n",
    "\r\n",
    "sequences = char_dataset.batch(seq_len + 1, drop_remainder=True)\r\n",
    "\r\n",
    "def create_seq_targets(seq):\r\n",
    "    # seq -- Hello my name \r\n",
    "    input_text = seq[:-1] # Hello my nam\r\n",
    "    target_text = seq[1:] # ello my name \r\n",
    "    return input_text, target_text \r\n",
    "\r\n",
    "dataset = sequences.map(create_seq_targets) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "for i in range(total_num_seq):\r\n",
    "    start = i * (seq_len + 1)\r\n",
    "    end = start + seq_len + 1 \r\n",
    "    print(encoded_text[start:end])"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for input_txt, target_txt in dataset.take(1): \r\n",
    "    print(input_txt.numpy()) \r\n",
    "    print(\"\".join(ind_to_char[input_txt.numpy()]))\r\n",
    "\r\n",
    "    print(target_txt.numpy())\r\n",
    "    print(\"\".join(ind_to_char[target_txt.numpy()]))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "batch_size = 128\r\n",
    "buffer_size = 10000\r\n",
    "dataset = dataset.shuffle(buffer_size=buffer_size).batch(batch_size=batch_size, drop_remainder=True) \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "vocab_size = len(vocab)\r\n",
    "vocab_size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "embed_dim = 64 \r\n",
    "rnn_neurons = 1026\r\n",
    "\r\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def sparse_cat_loss(y_true, y_pred):\r\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from tensorflow.keras.models import Sequential \r\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\r\n",
    "    model = Sequential()\r\n",
    "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape = [batch_size, None])) \r\n",
    "    model.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\r\n",
    "    model.add(Dense(vocab_size))\r\n",
    "\r\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss)\r\n",
    "\r\n",
    "    return model "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "model = create_model(vocab_size=vocab_size, embed_dim=embed_dim, rnn_neurons=rnn_neurons, batch_size=batch_size)\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 64)           5376      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (128, None, 1026)         3361176   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 84)           86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4829509429bc936f3154465d610ee2a3576f51308e11c88f126dfe38dafbbc67"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}