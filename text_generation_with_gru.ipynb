{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PART 1 \r\n",
    "# reading the data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np \r\n",
    "import pandas as pd \r\n",
    "import matplotlib.pyplot as plt \r\n",
    "import tensorflow as tf "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# opening the file \r\n",
    "path_to_file = 'shakespeare.txt' \r\n",
    "with open(path_to_file,'r') as f:\r\n",
    "    text = f.read() \r\n",
    "\r\n",
    "# creating a vocabulary list \r\n",
    "vocab = sorted(set(text))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "len(vocab) # length of the vocabulary "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART 2 \r\n",
    "# test processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# we want to assign a number to every character in the vocabulary \r\n",
    "char_to_ind = {char:ind for ind,char in enumerate(vocab)} # characters to index \r\n",
    "ind_to_char = np.array(vocab) # index to characters \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "encoded_text = np.array([char_to_ind[char] for char in text])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "seq_len = 120 \r\n",
    "total_num_seq = len(text)  // (seq_len + 1)\r\n",
    "total_num_seq\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "45005"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#char_dataset.batch()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#for item in char_dataset.take(500):\r\n",
    "#    print(ind_to_char[item.numpy()])\r\n",
    "\r\n",
    "sequences = char_dataset.batch(seq_len + 1, drop_remainder=True)\r\n",
    "\r\n",
    "def create_seq_targets(seq):\r\n",
    "    # seq -- Hello my name \r\n",
    "    input_text = seq[:-1] # Hello my nam\r\n",
    "    target_text = seq[1:] # ello my name \r\n",
    "    return input_text, target_text \r\n",
    "\r\n",
    "dataset = sequences.map(create_seq_targets) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "for i in range(total_num_seq):\r\n",
    "    start = i * (seq_len + 1)\r\n",
    "    end = start + seq_len + 1 \r\n",
    "    print(encoded_text[start:end])"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for input_txt, target_txt in dataset.take(1): \r\n",
    "    print(input_txt.numpy()) \r\n",
    "    print(\"\".join(ind_to_char[input_txt.numpy()]))\r\n",
    "\r\n",
    "    print(target_txt.numpy())\r\n",
    "    print(\"\".join(ind_to_char[target_txt.numpy()]))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "batch_size = 128\r\n",
    "buffer_size = 10000\r\n",
    "dataset1 = dataset.shuffle(buffer_size=buffer_size).batch(batch_size=batch_size, drop_remainder=True) \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "vocab_size = len(vocab)\r\n",
    "vocab_size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "embed_dim = 64 \r\n",
    "rnn_neurons = 1026\r\n",
    "\r\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def sparse_cat_loss(y_true, y_pred):\r\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from tensorflow.keras.models import Sequential \r\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\r\n",
    "    model = Sequential()\r\n",
    "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape = [batch_size, None])) \r\n",
    "    model.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\r\n",
    "    model.add(Dense(vocab_size))\r\n",
    "\r\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss)\r\n",
    "\r\n",
    "    return model "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "model = create_model(vocab_size=vocab_size, embed_dim=embed_dim, rnn_neurons=rnn_neurons, batch_size=batch_size)\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 64)           5376      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (128, None, 1026)         3361176   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 84)           86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "for input_example_batch, target_example_batch in dataset1.take(1):\r\n",
    "    example_batch_predictions = model(input_example_batch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "input_example_batch[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(120,), dtype=int32, numpy=\n",
       "array([59, 77, 64, 74, 60,  1, 80, 70, 76,  1, 61, 76, 73, 75, 63, 60, 73,\n",
       "       22,  1, 57, 76, 75,  1, 34,  0,  1,  1,  1,  1, 63, 70, 71, 60,  1,\n",
       "       80, 70, 76, 73,  1, 70, 78, 69,  1, 62, 73, 56, 58, 60,  1, 78, 64,\n",
       "       67, 67,  1, 66, 60, 60, 71,  1, 80, 70, 76,  1, 78, 63, 60, 73, 60,\n",
       "        1, 80, 70, 76,  1, 56, 73, 60,  8,  1, 75, 63, 70, 76, 62, 63,  1,\n",
       "       75, 63, 60, 73, 60,  0,  1,  1,  1,  1, 78, 60, 73, 60,  1, 69, 70,\n",
       "        1, 61, 76, 73, 75, 63, 60, 73,  1, 59, 56, 69, 62, 60, 73,  1, 66,\n",
       "       69])>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "example_batch_predictions.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([128, 120, 84])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "example_batch_predictions[0].numpy().argmax(axis = 1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([42, 37,  5, 42,  9,  1, 24, 77,  0,  1, 75, 75, 13, 60,  4,  9, 13,\n",
       "       50,  1, 49, 13, 60,  1,  1, 40, 15,  1,  1,  1,  7, 77, 10,  9, 15,\n",
       "       24, 77,  0, 13,  1, 77, 32,  4,  1,  1, 13, 26, 26, 55, 24, 67, 70,\n",
       "       67,  8, 38, 79,  0,  0, 11, 49, 24, 77,  0,  1, 32, 67,  9, 13,  9,\n",
       "       34, 24, 77,  0,  1,  7, 13,  9, 16, 47, 49,  4, 77, 83, 40,  4,  1,\n",
       "       49,  4,  9, 13,  9, 70, 15, 15,  1,  1, 49,  8, 13, 55, 34, 47, 77,\n",
       "        1,  4, 75, 13, 60,  4,  9, 13,  1, 24, 37,  4, 54,  9, 13, 13, 58,\n",
       "       59], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "ind_to_char[example_batch_predictions[0].numpy().argmax(axis = 1)]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Q', 'L', \"'\", 'Q', '-', ' ', '>', 'v', '\\n', ' ', 't', 't', '2',\n",
       "       'e', '&', '-', '2', 'Y', ' ', 'X', '2', 'e', ' ', ' ', 'O', '4',\n",
       "       ' ', ' ', ' ', ')', 'v', '.', '-', '4', '>', 'v', '\\n', '2', ' ',\n",
       "       'v', 'G', '&', ' ', ' ', '2', 'A', 'A', '`', '>', 'l', 'o', 'l',\n",
       "       ',', 'M', 'x', '\\n', '\\n', '0', 'X', '>', 'v', '\\n', ' ', 'G', 'l',\n",
       "       '-', '2', '-', 'I', '>', 'v', '\\n', ' ', ')', '2', '-', '5', 'V',\n",
       "       'X', '&', 'v', '}', 'O', '&', ' ', 'X', '&', '-', '2', '-', 'o',\n",
       "       '4', '4', ' ', ' ', 'X', ',', '2', '`', 'I', 'V', 'v', ' ', '&',\n",
       "       't', '2', 'e', '&', '-', '2', ' ', '>', 'L', '&', '_', '-', '2',\n",
       "       '2', 'c', 'd'], dtype='<U1')"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#epochs = 30\r\n",
    "#model.fit(dataset1, epochs=epochs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from tensorflow.keras.models import load_model \r\n",
    "model = create_model(vocab_size=vocab_size, embed_dim=embed_dim, rnn_neurons=rnn_neurons, batch_size=1)\r\n",
    "model.load_weights('shakespeare_gen.h5')\r\n",
    "model.build(tf.TensorShape([1,None]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (1, None, 64)             5376      \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (1, None, 1026)           3361176   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (1, None, 84)             86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "def generate_text(model, start_seed, gen_size = 500, temp = 1.0):\r\n",
    "    pass \r\n",
    "model = model \r\n",
    "start_seed = \"\"\"So is it not with me as with that muse,\r\n",
    "Stirred by a painted beauty to his verse,\"\"\"\r\n",
    "gen_size = 500 \r\n",
    "temp = 1.0 \r\n",
    "\r\n",
    "num_generate = gen_size \r\n",
    "\r\n",
    "input_eval = [char_to_ind[char] for char in start_seed]\r\n",
    "\r\n",
    "input_eval = tf.expand_dims(input_eval,0)\r\n",
    "\r\n",
    "text_generated = [] \r\n",
    "\r\n",
    "temperature = temp \r\n",
    "# you should call reset_states every time, when you want to make consecutive model calls independent\r\n",
    "model.reset_states()\r\n",
    "\r\n",
    "for i in range(num_generate): \r\n",
    "  #print(1)\r\n",
    "  # this for loop is for number of characters we are about to generate \r\n",
    "\r\n",
    "  predictions = model(input_eval) \r\n",
    "  predictions = tf.squeeze(predictions,axis = 0)\r\n",
    "\r\n",
    "  predictions = predictions / temperature \r\n",
    "  predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n",
    "  print(ind_to_char[predicted_id])\r\n",
    "  input_eval = tf.expand_dims([predicted_id],0) \r\n",
    "  text_generated.append(ind_to_char[predicted_id]) \r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      " \n",
      " \n",
      "A\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "y\n",
      " \n",
      "h\n",
      "a\n",
      "n\n",
      "d\n",
      "s\n",
      " \n",
      "l\n",
      "o\n",
      "r\n",
      "d\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "d\n",
      "s\n",
      ",\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "k\n",
      "i\n",
      "l\n",
      "l\n",
      "'\n",
      "d\n",
      " \n",
      "g\n",
      "i\n",
      "v\n",
      "i\n",
      "n\n",
      "g\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "F\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "m\n",
      "e\n",
      "g\n",
      "h\n",
      "a\n",
      "i\n",
      "t\n",
      " \n",
      "o\n",
      "n\n",
      "'\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "L\n",
      "e\n",
      "o\n",
      "n\n",
      ".\n",
      " \n",
      "N\n",
      "o\n",
      ",\n",
      " \n",
      "m\n",
      "y\n",
      " \n",
      "l\n",
      "o\n",
      "r\n",
      "d\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "s\n",
      "o\n",
      "r\n",
      "r\n",
      "o\n",
      "w\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "d\n",
      "s\n",
      "y\n",
      ".\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "I\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "u\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      " \n",
      "f\n",
      "e\n",
      "a\n",
      "r\n",
      "f\n",
      "u\n",
      "l\n",
      " \n",
      "m\n",
      "o\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      "t\n",
      "h\n",
      "b\n",
      "u\n",
      "l\n",
      "s\n",
      "t\n",
      "\n",
      "\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "u\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "t\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "w\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "p\n",
      "o\n",
      "s\n",
      "s\n",
      "i\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "I\n",
      "s\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "h\n",
      "a\n",
      "r\n",
      "b\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "r\n",
      "i\n",
      "c\n",
      "k\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "f\n",
      "a\n",
      "c\n",
      "e\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "C\n",
      "A\n",
      "S\n",
      "S\n",
      "I\n",
      "O\n",
      ".\n",
      " \n",
      "N\n",
      "o\n",
      ",\n",
      " \n",
      "m\n",
      "y\n",
      " \n",
      "o\n",
      "l\n",
      "d\n",
      " \n",
      "s\n",
      "t\n",
      "r\n",
      "e\n",
      "n\n",
      "g\n",
      "t\n",
      "h\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "w\n",
      "i\n",
      "s\n",
      "d\n",
      "o\n",
      "m\n",
      "s\n",
      " \n",
      "c\n",
      "r\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "'\n",
      "F\n",
      "o\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      "f\n",
      "o\n",
      "r\n",
      "e\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "I\n",
      " \n",
      "c\n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "w\n",
      "h\n",
      "a\n",
      "t\n",
      "'\n",
      "s\n",
      " \n",
      "a\n",
      " \n",
      "r\n",
      "a\n",
      "i\n",
      "l\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "p\n",
      "a\n",
      "s\n",
      "s\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "c\n",
      "l\n",
      "o\n",
      "c\n",
      "k\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "I\n",
      " \n",
      "d\n",
      "i\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "k\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "h\n",
      "i\n",
      "m\n",
      ";\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "s\n",
      "h\n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "s\n",
      "e\n",
      "e\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      "s\n",
      "u\n",
      "r\n",
      "d\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "A\n",
      "g\n",
      "a\n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      " \n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "f\n",
      "r\n",
      "i\n",
      "e\n",
      "n\n",
      "d\n",
      ".\n",
      " \n",
      "G\n",
      "o\n",
      "o\n",
      "d\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      "a\n",
      "y\n",
      ",\n",
      " \n",
      "w\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      ";\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "A\n",
      "n\n",
      "d\n",
      ",\n",
      " \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "start_seed + \"\".join(text_generated)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"So is it not with me as with that muse,\\nStirred by a painted beauty to his verse,\\n  And fools doth this but bid ARATED OTHERS\\nPERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED\\nCOMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY\\nSERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>\\n\\n\\n\\nSCENE 2\\n\\nThe Lord's Rode?\\n  AJAX. Do you but perceive to pieces?\\n  EVANS. Is 'tes you?\\n  CLOWN. Nay, but notople her.\\n  VOLUMNIA. He says he, sir, but hear me swear, contrives no  ve not speak a\\n    cockagent, and there's courage and a braver one. If I live press, and sp\""
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "predicted_id"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "input_eval = tf.expand_dims([predicted_id],0)\r\n",
    "input_eval"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]])>"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "ind_to_char[predicted_id]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "tf.random.categorical(tf.squeeze(model(input_eval),0), num_samples=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[44]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "ind_to_char[44]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "def generate_text(model, start_seed, gen_size = 500, temp = 1.0):\r\n",
    "    num_generate = gen_size \r\n",
    "\r\n",
    "    input_eval = [char_to_ind[char] for char in start_seed] \r\n",
    "    input_eval = tf.expand_dims(input_eval,0) # adding a dimension before \r\n",
    "\r\n",
    "    text_generated = [] \r\n",
    "\r\n",
    "    temperature = temp \r\n",
    "\r\n",
    "    model.reset_states() \r\n",
    "\r\n",
    "    for i in range(num_generate): \r\n",
    "        predictions = model(input_eval) \r\n",
    "        predictions = tf.squeeze(predictions, axis = 0)\r\n",
    "        predictions = predictions / temperature \r\n",
    "        prediction_id = predictions.numpy().argmax(axis = 1)[-1]\r\n",
    "        #prediction_id  = tf.random.categorical(prediction_id, num_samples=1)[-1,0].numpy() \r\n",
    "        input_eval = tf.expand_dims([prediction_id],0)\r\n",
    "        text_generated.append(ind_to_char[prediction_id]) \r\n",
    "    return start_seed + \"\".join(text_generated)\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "start_seed = \"\"\"romeo and juliet\"\"\"\r\n",
    "generate_text(model, start_seed, gen_size = 500, temp = 1.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4829509429bc936f3154465d610ee2a3576f51308e11c88f126dfe38dafbbc67"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}