{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "path_to_file = 'shakespeare.txt' \r\n",
    "with open(path_to_file,'r') as f: \r\n",
    "    text = f.read()\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "vocab = sorted(set(text))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "char_to_ind = {u:i for i, u in enumerate(vocab)} \r\n",
    "ind_to_char = np.array(vocab) \r\n",
    "encoded_text = np.array([char_to_ind[char] for char in text]) \r\n",
    "encoded_text"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0,  1,  1, ..., 30, 39, 29])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "seq_len = 120 \r\n",
    "total_num_seq = len(text) // (seq_len + 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# creatimg training sequences \r\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\r\n",
    "sequences = char_dataset.batch(seq_len + 1, drop_remainder=True) \r\n",
    "\r\n",
    "def create_seq_targets(seq):\r\n",
    "    input_txt = seq[:-1]\r\n",
    "    target_txt = seq[1:]\r\n",
    "\r\n",
    "    return input_txt, target_txt \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dataset = sequences.map(create_seq_targets) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "batch_size = 128 \r\n",
    "buffer_size = 10000 \r\n",
    "dataset = dataset.shuffle(buffer_size=buffer_size).batch(batch_size, drop_remainder=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# length of the vocabulary in chars \r\n",
    "vocab_size = len(vocab) \r\n",
    "# the embedding dimension \r\n",
    "embed_dim = 64 \r\n",
    "# number of rnn units \r\n",
    "rnn_neurons = 1026 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from tensorflow.keras.models import Sequential \r\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU \r\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def sparse_cat_loss(y_true,y_pred):\r\n",
    "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\r\n",
    "  \r\n",
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\r\n",
    "    model = Sequential()\r\n",
    "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\r\n",
    "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\r\n",
    "    # Final Dense Layer to Predict\r\n",
    "    model.add(Dense(vocab_size))\r\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss) \r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model = create_model(\r\n",
    "    vocab_size=vocab_size, \r\n",
    "    embed_dim=embed_dim,\r\n",
    "    rnn_neurons=rnn_neurons,\r\n",
    "    batch_size=batch_size\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 64)           5376      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (128, None, 1026)         3361176   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 84)           86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from tensorflow.keras.models import load_model\r\n",
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\r\n",
    "\r\n",
    "model.load_weights('shakespeare_gen.h5')\r\n",
    "\r\n",
    "model.build(tf.TensorShape([1, None]))\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def generate_text(model, start_seed, gen_size = 100, temp = 1.0): \r\n",
    "    num_generate = gen_size \r\n",
    "\r\n",
    "    input_eval = [char_to_ind[s] for s in start_seed] \r\n",
    "    input_eval = tf.expand_dims(input_eval,0) \r\n",
    "\r\n",
    "    text_generated = [] \r\n",
    "\r\n",
    "    temperature = temp \r\n",
    "\r\n",
    "    model.reset_states()\r\n",
    "\r\n",
    "    for i in range(num_generate): \r\n",
    "        predictions = model(input_eval) \r\n",
    "\r\n",
    "        predictions = model(input_eval) \r\n",
    "\r\n",
    "        predictions = tf.squeeze(predictions,0) \r\n",
    "\r\n",
    "        predictions = predictions / temperature \r\n",
    "\r\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n",
    "\r\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\r\n",
    "\r\n",
    "        text_generated.append(ind_to_char[predicted_id]) \r\n",
    "\r\n",
    "\r\n",
    "    return start_seed + ''.join(text_generated)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4829509429bc936f3154465d610ee2a3576f51308e11c88f126dfe38dafbbc67"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}